<!DOCTYPE html>


  <html class="light page-post">


<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Paper Notes on Learning Deep Representations by Mutual Information Estimation and Maximization | Deep Mental Artificial Intelligence</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="Paper Notes,">
  

  <meta name="description" content="OverviewIntroductionOne core objective of deep learning is to discover useful representations. This paper explores the idea of maximizing the mutual information (MI) between the inputs and outputs of">
<meta name="keywords" content="Paper Notes">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Notes on Learning Deep Representations by Mutual Information Estimation and Maximization">
<meta property="og:url" content="https://zhenyueqin.github.io/2019/02/12/paper-notes-deep-info-max/index.html">
<meta property="og:site_name" content="Deep Mental Artificial Intelligence">
<meta property="og:description" content="OverviewIntroductionOne core objective of deep learning is to discover useful representations. This paper explores the idea of maximizing the mutual information (MI) between the inputs and outputs of">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-04-01T07:00:31.709Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Paper Notes on Learning Deep Representations by Mutual Information Estimation and Maximization">
<meta name="twitter:description" content="OverviewIntroductionOne core objective of deep learning is to discover useful representations. This paper explores the idea of maximizing the mutual information (MI) between the inputs and outputs of">

  

  
    <link rel="icon" href="/images/general/Hua-Round.png">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  

  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  

</head>
</html>
<body>


  
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/archives/" rel="noopener noreferrer" target="_self">
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/category/" rel="noopener noreferrer" target="_self">
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/tag/" rel="noopener noreferrer" target="_self">
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/link/" rel="noopener noreferrer" target="_self">
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/about/" rel="noopener noreferrer" target="_self">
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/search/" rel="noopener noreferrer" target="_self">
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>




<div class="content content-post CENTER">
   <article id="post-paper-notes-deep-info-max" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">Paper Notes on Learning Deep Representations by Mutual Information Estimation and Maximization</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2019.02.12</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Zhenyue Qin</span>
        </span>
      

      
  <span class="article-category">
    <i class="icon-list"></i>
    <a class="article-category-link" href="/categories/Computer-Vision/">Computer Vision</a>
  </span>



      

      
      
    </div>
  </header>

  <div class="article-content">
    
      <h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>One core objective of deep learning is to discover useful representations. This paper explores the idea of maximizing the mutual information (MI) between the inputs and outputs of an encoder. Importantly, structure matters. It can substantially improve the representation’s quality by maximizing the average MI between local regions of inputting images and representations <sup><a href="#fn_T1" id="reffn_T1">T1</a></sup>, while gloabl MI computed from the entire input plays a stronger role in the reconstruction the full input given the representation. </p>
<p>The method in this paper is called Deep InfoMax (DIM) since it is closely related to the <em>information maximiazation principle</em> <sup><a href="#fn_Q1" id="reffn_Q1">Q1</a></sup>. This paper combines MI maximization with prior matching in a manner similar to <em>adversarial autoencoders</em> <sup><a href="#fn_Q2" id="reffn_Q2">Q2</a></sup>. The main contributions are the following: </p>
<ol>
<li>The formulation of Deep InfoMax (DIM), which simultaneously estimates and maximizes the MI between input data and high-level representaions <sup><a href="#fn_Q3" id="reffn_Q3">Q3</a></sup>. </li>
<li>Various priorities on global and local inputting information of MI, which can be turned for the suitability for classification and reconstruction-style tasks. </li>
<li>Usage of adversarial training to make the representation to have desired statistical characteristics <sup><a href="#fn_Q4" id="reffn_Q4">Q4</a></sup> specific to a prior.  </li>
<li>Introduction of two new metrics on evaluating representations, one based on Mutual Information Neural Estimation (MINE) and a neural dependency measure (NDM). </li>
</ol>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="MI-and-Generative-Models"><a href="#MI-and-Generative-Models" class="headerlink" title="MI and Generative Models"></a>MI and Generative Models</h3><p>The reconstruction error can be related to MI as follows <sup><a href="#fn_Q5" id="reffn_Q5">Q5</a></sup>, where $e$ and $d$ stands for encoding and decoding respectively: </p>
<script type="math/tex; mode=display">
I_e(X, Y) = H_e(X) - H_e(X|Y) \geq H_e(X) - R_{e, d}(X | Y)</script><p>where $X$ and $Y$ denote the inputs and outputs of the encoder which is applied to inputs sampled from some source distribution. $R_{e, d}$ denotes the expected reconstruction error given the codes of $Y$. </p>
<h3 id="Mutual-information-Estimation"><a href="#Mutual-information-Estimation" class="headerlink" title="Mutual-information Estimation"></a>Mutual-information Estimation</h3><p>In typical settings, models with reconstruction-typed objectives provide some guarantees on the amount of information encoded in their intermediate representations <sup><a href="#fn_Q6" id="reffn_Q6">Q6</a></sup>. Similar guarantees exist for bi-directional adversarial models, which adversarially train an encoder/decoder to match their respective joint distributions or to minimize the reconstruction error <sup><a href="#fn_Q7" id="reffn_Q7">Q7</a></sup>. </p>
<p>The infomax principle <sup><a href="#fn_F1" id="reffn_F1">F1</a></sup> advocates maximizing MI between the input and output. MINE learns an estimate of MI for continuous variables, which is strongly consistent and can be used to learn better implicitly bi-diretional generative models <sup><a href="#fn_Q8" id="reffn_Q8">Q8</a></sup>. </p>
<p>DIM follows the spirit of MINE in this regard, though DIM finds it is unnecessary to use a generator, and also unnecessary to use the exact KL-based formulation of MI. Instead, a simple alternative based on the Jenson-Shannon divergence is more stable and provides better results. </p>
<p>More significantly, DIM can leverage local structure in the input to improve the suitbality of representations for classifications <sup><a href="#fn_Q9" id="reffn_Q9">Q9</a></sup>. </p>
<p>It has been shown in the case of discrete MI that data augmentations and other transformations can be used to avoid degenerate solutions <sup><a href="#fn_Q10" id="reffn_Q10">Q10</a></sup>. </p>
<p>Proposed independently of DIM, Contrastive Predictive Coding is a MI-based approach that, like DIM, maximizes MI between gloabl and local representation pairs. CPC shares some motivations and computations with DIM, but there are important ways in which CPC and DIM differ. CPC processes local features sequentially to build partial “summary features”, which are used to make predictions about specific local features in the “future” of each summary feature. This equates to ordered autoregression over the local features, and requires training separate estimators for each temporal offset at which one would like to predict the future. In contrast, the basic version of DIM uses a single summary feature that is a function of all local features, and this “global” feature predicts all local features simultaneously in a single step using a single estimator. Note that, when using occlusions during training, DIM performs both “self” predictions and orderless autoregression. <sup><a href="#fn_Q11" id="reffn_Q11">Q11</a></sup></p>
<h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p>Marginal distribution of latent variable $y$ <sup><a href="#fn_Q13" id="reffn_Q13">Q13</a></sup></p>
<script type="math/tex; mode=display">
\begin{align}
U_{\psi, P}(y)     &= \sum_{x \in X} p(y | x) p(x) \\
                &= \sum_{i=1}^{N} p(y | x) (\frac{\delta(x)}{N})
\end{align}</script><script type="math/tex; mode=display">
L_{\hat{w}, \hat{\psi}} = \underset{\hat{w}, \hat{\psi}} {\arg \max} \frac{1}{M^{2}} \sum_{i=1}^{M^2} \hat{I} _ {\hat{w}, \hat{\psi}} (C_{\psi}^{i} (X) ; E_{\psi}(X))</script><p><sup><a href="#fn_Q12" id="reffn_Q12">Q12</a></sup></p>
<script type="math/tex; mode=display">
\hat{D} _ {\phi} (V || U) = \underset{\psi} {\arg\min} \text{ } \underset{\phi} {\arg\max} E_{V}[\log D_{\phi}(y)] + E_{P}[\log (1 - D_{\phi}{ (E_{\psi}(x)) })]</script><p>Note that the parameter $w$ is not involved in calculating the distance between $V$ and $U$. </p>
<p>Overall objective</p>
<script type="math/tex; mode=display">
\underset{w_1, w_2, \phi}{\arg\max} \big( 
\alpha I_{\hat{w_1}, \hat{\phi}} (X; E_{\phi} (X)) + 
\frac{\beta}{M^{2}} \sum_{i=1}^{M^2} \hat{I} _ {\hat{w_2}, \hat{\psi}} (C_{\psi}^{i} (X) ; E_{\psi}(X))
\big) 
+ \underset{\psi} {\arg\min} \text{ } \underset{\phi} {\arg\max} \hat{D} _ {\phi} (V || U)</script><h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="Thoughts"><a href="#Thoughts" class="headerlink" title="Thoughts"></a>Thoughts</h2><p><sup><a href="#fn_T1" id="reffn_T1">T1</a></sup> Can we follow the same spirit of Mask R-CNN to propose RoIs? </p>
<h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><p><sup><a href="#fn_F1" id="reffn_F1">F1</a></sup> Read “infomax principle”. </p>
<h2 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h2><p><sup><a href="#fn_Q1" id="reffn_Q1">Q1</a></sup> What is “information maximization principle”?<br><sup><a href="#fn_Q2" id="reffn_Q2">Q2</a></sup> What is “adversarial autoencoders”?<br><sup><a href="#fn_Q3" id="reffn_Q3">Q3</a></sup> What are those “high-level representations”?<br><sup><a href="#fn_Q4" id="reffn_Q4">Q4</a></sup> What is a “desired statistics”?<br><sup><a href="#fn_Q5" id="reffn_Q5">Q5</a></sup> I don’t understand the equation.<br><sup><a href="#fn_Q6" id="reffn_Q6">Q6</a></sup> Why?<br><sup><a href="#fn_Q7" id="reffn_Q7">Q7</a></sup> I don’t understand this sentence at all.<br><sup><a href="#fn_Q8" id="reffn_Q8">Q8</a></sup> What are “implicitly bi-directional generative models”?<br><sup><a href="#fn_Q9" id="reffn_Q9">Q9</a></sup> Only classifications.<br><sup><a href="#fn_Q10" id="reffn_Q10">Q10</a></sup> What are “degenerated solutions”?<br><sup><a href="#fn_Q11" id="reffn_Q11">Q11</a></sup> I don’t quite understand the paragraph about CPC.<br><sup><a href="#fn_Q12" id="reffn_Q12">Q12</a></sup> Can we make $C_{\psi}^{i}$ as different as possible?<br><sup><a href="#fn_Q13" id="reffn_Q13">Q13</a></sup> How to solve this by SGD. </p>

    
  </div>

</article>


   

   



</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">Close</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/archives/" rel="noopener noreferrer" target="_self">
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/category/" rel="noopener noreferrer" target="_self">
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/tag/" rel="noopener noreferrer" target="_self">
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/link/" rel="noopener noreferrer" target="_self">
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/about/" rel="noopener noreferrer" target="_self">
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/search/" rel="noopener noreferrer" target="_self">
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <div id="comment" class="vcomment"></div>
    <script>
        var notify = 'true' == true ? true : false;
        var verify = 'true' == true ? true : false;
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
            return GUEST_INFO.indexOf(item) > -1
        });
        guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
        window.valine = new Valine({
            el: '.vcomment',
            notify: notify,
            verify: verify,
            appId: "",
            appKey: "",
            avatar:'mm',
            placeholder: "Just go go",
            guest_info:guest_info,
            pageSize:'10'
        });
    </script>
  
    

  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
