<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="author" content="Zhenyue Qin"><link rel="alternative" href="/atom.xml" title="Deep Neuropathic Network" type="application/atom+xml"><link rel="icon" href="/images/general/Hua-Round.png"><title>Paper Notes on Auto-Encoding Generative Adversarial Network - Deep Neuropathic Network</title><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/js/fancybox/jquery.fancybox.min.css"><!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]--><script src="/js/jquery-3.1.1.min.js"></script><script src="/js/fancybox/jquery.fancybox.min.js"></script></head><body style="opacity:0"><header class="head"><h1 class="head-title u-fl"><a href="/">Deep Neuropathic Network</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a class="head-nav__link" href="/archives">Articles</a></li><li class="head-nav__item"><a class="head-nav__link" href="/research">Research</a></li><li class="head-nav__item"><a class="head-nav__link" href="/music">Music</a></li><li class="head-nav__item"><a class="head-nav__link" href="/about">About</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time class="post__time" datetime="2019-02-07T06:42:37.000Z">2019 - 02 - 07 17:42:37</time><h1 class="post__title"><a href="/2019/02/07/paper-notes-auto-encoding-gan/">Paper Notes on Auto-Encoding Generative Adversarial Network</a></h1><div class="post__main echo"><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>This paper addresses a general framework for combining VAEs and GANs. </p>
<h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h2><ul>
<li>$p^{\ast}(x)$ - The true data distribution. </li>
<li>$p_{\theta}(x)$ - The reconstructed data from the latent variables, parameter $\theta$ is for decoding networks. </li>
<li>$D_{\phi}(x)$ - Data discriminator, parameter $\phi$ is for discriminating data. </li>
<li>$C_{w}(x)$ - Latent variable discriminator, parameter $w$ is for discriminating latent variables.</li>
<li>$G_{\theta}(x)$ - Decoding networks. </li>
<li>$q_{\eta}(z | x)$ - The symbol $\eta$ is for the inference distribution $q$.</li>
</ul>
<h2 id="Generative-Adversarial-Networks"><a href="#Generative-Adversarial-Networks" class="headerlink" title="Generative Adversarial Networks"></a>Generative Adversarial Networks</h2><p><strong>Discriminator Loss</strong>: </p>
<script type="math/tex; mode=display">E_{p^{\ast}(x)} [-\log(D_{\phi}(x)) + E_{p_{\theta}(x)}[-\log(1-D_{\phi}(x))]</script><p><strong>Generator Loss</strong>: </p>
<script type="math/tex; mode=display">E_{p_{\theta}(x)} [\log(1-D_{\phi}(x))]</script><p>However, this generator loss may cause the non-saturating problem at the incipient training phase, as a perfect discriminator will make the generator losing the gradient. We therefore adopt the alternative generator loss to provide stronger gradients. </p>
<p><strong>Alternative Generator Loss</strong>: </p>
<script type="math/tex; mode=display">E_{p_{\theta}(x)} [-\log(D_{\phi}(x))]</script><h2 id="Density-Ratio-Trick"><a href="#Density-Ratio-Trick" class="headerlink" title="Density Ratio Trick"></a>Density Ratio Trick</h2><p>By introducing the lables of $y=1$ for real data and $y=0$ for fake data, with Bayesâ€™ Theorem and an assumption that the two marginal probabilities are equal, the density ratio between the model and real data distributions $r_{\phi}(x)$ can be computed with</p>
<script type="math/tex; mode=display">
r_{\phi}(x) = \frac{p^{\ast}(x)}{p_{\theta}(x)} = \frac{p(x | y = 1)}{p(x | y = 0)} = \frac{p(y=1 | x)}{p(y=0 | x)} = \frac{D_{\phi}(x)}{1 - D_{\phi}(x)}</script><p>We aim to implicitly derive the distribution of input $x$ the same as the distribution of the real data, which implies $r_{\phi}(x) = 1$. Thus, ideally we have</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{D_{\phi}(x)}{1 - D_{\phi}(x)} = 1 \Rightarrow D_{\phi}(x) = \frac{1}{2}
\end{aligned}</script><p>which suggests the termination signal of GAN training is when the discriminator cannot distinguish real and fake inputs. </p>
<p>This tells us that whenever we wish to compute the density ratio, we can simply draw samples from the two distributions and implement a binary classifier $D_{\phi}(x)$ of the two sets of samples. By using the density ratio, GANs account for the intractability of the margional likelihood by only looking at the relative behaviors beween the model and true data distributions. This trick only requires sampling from the two distributions and never requires accessing analytical forms of the distributions. </p>
<h2 id="Implicit-Variational-Distributions"><a href="#Implicit-Variational-Distributions" class="headerlink" title="Implicit Variational Distributions"></a>Implicit Variational Distributions</h2><p>The major task of variational inferences is the choice of approximate posterior distribution $q_{\eta}(z | x)$. Common approaches assume it to be a Gaussian. We would not make a restrictive choice of distribution by making it implicit and using the density ratio trick. We train a latent classifier $C_w$ which discriminates between latent variables $z$ produced from a encoding network and variables sampled from a standard Gaussian distribution. </p>
<script type="math/tex; mode=display">
-D_{KL} [q_{\eta}(z | x) || p(z)] = E_{q_{\eta}(z | x)}[\log \frac{p(z)}{q_{\eta}(z | x)}] \approx E_{q_{\eta}(z | x)}[\log \frac{C_w(x)}{1 - C_{w}(x)}]</script><h2 id="Mode-Collapse-Problem"><a href="#Mode-Collapse-Problem" class="headerlink" title="Mode Collapse Problem"></a>Mode Collapse Problem</h2><p>Inferring distributions with density ratio tricks can lead to the mode collapse problem with not putting probability mass to all the distributions in the data space. We address this problem by introducing a zero-mean Laplacian distribution </p>
<script type="math/tex; mode=display">
p_{\theta}(x | z) \propto \exp \{- \lambda || x - G_{\theta}(x) ||_1 \}</script><p>which corresponds to using a variational autoencoder with an $L_{1}$ loss. </p>
<h2 id="Hybrid-Loss-Functions"><a href="#Hybrid-Loss-Functions" class="headerlink" title="Hybrid Loss Functions"></a>Hybrid Loss Functions</h2><p>An hybrid objective function that combines all these choices is to maximize</p>
<script type="math/tex; mode=display">
L(\theta, \eta) = E_{q_{\eta}(z | x)}[ -\lambda || x - G_{\theta}(x) ||_1  + \log \frac{D_{\phi}(G(\theta))}{1 - D_{\phi}(G(\theta))} + \log \frac{C_{w}(z)}{1 - C_{w}(z)}]</script><h2 id="Improved-Techniques"><a href="#Improved-Techniques" class="headerlink" title="Improved Techniques"></a>Improved Techniques</h2><p>The loss for a generator is to minimize</p>
<script type="math/tex; mode=display">
E_{q_{\eta}(z | x)} [ \lambda || x - G_{\theta}(x) ||_1 - \log D_{\phi}(G(z)) + \log (1 - D_{\phi}(G(z))) ]</script><!-- <img src="/2019/02/07/paper-notes-auto-encoding-gan/alpha_gan_architecture.jpg" title="Alpha GAN Architecture"> -->
<h2 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h2><center><img src="/2019/02/07/paper-notes-auto-encoding-gan/alpha_gan_architecture.jpg" alt="Alpha GAN Architecture" width="400"></center>
</div></header><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a class="post__tag__link" href="/tags/Paper-Notes/">Paper Notes</a></li></ul></footer></article><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="//cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></main><footer class="foot"><div class="foot-copy">&copy; 1994 - 2020 Zhenyue Qin</div></footer><script src="/js/scroller.js"></script><script src="/js/main.js"></script></body></html>