<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="author" content="Zhenyue Qin"><link rel="alternative" href="/atom.xml" title="Deep Mental Artificial Intelligence" type="application/atom+xml"><link rel="icon" href="/images/general/Hua-Round.png"><title>Paper Notes on A Variational U-Net for Conditional Appearance and Shape Generation - Deep Mental Artificial Intelligence</title><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/js/fancybox/jquery.fancybox.min.css"><!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]--><script src="/js/jquery-3.1.1.min.js"></script><script src="/js/fancybox/jquery.fancybox.min.js"></script></head><body style="opacity:0"><header class="head"><h1 class="head-title u-fl"><a href="/">Deep Mental Artificial Intelligence</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a class="head-nav__link" href="/archives">目录</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time class="post__time" datetime="2019-02-08T11:08:39.000Z">2019 - 02 - 08 22:08:39</time><h1 class="post__title"><a href="/2019/02/08/paper-notes-variational-u-net/">Paper Notes on A Variational U-Net for Conditional Appearance and Shape Generation</a></h1><div class="post__main echo"><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>This paper develops a conditional U-Net for shape-guided image generation, employing variational inference for appearance. That is, it disentangles an image into shape and appearance. </p>
<h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Variational-Autoencoder-with-Latent-Shape-and-Appearance"><a href="#Variational-Autoencoder-with-Latent-Shape-and-Appearance" class="headerlink" title="Variational Autoencoder with Latent Shape and Appearance"></a>Variational Autoencoder with Latent Shape and Appearance</h2><p>We have the Evidence Lower Bound Observation (ELBO), with Jenson’s Inequality</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\log p(x)    &= \log \int p(x, y, z) dy dz \\
            &= \log \int \frac{p(x, y, z)}{q(y, z | x)} q(y, z | x) dy dz \\ 
            &= \log E_{q(y,z | x)} \frac{p(x, y, z)}{q(y, z | x)} \\ 
            &\geq E_{q(y,z | x)} \log \frac{p(x, y, z)}{q(y, z | x)} \\ 
            &= E_{q(y,z | x)} \log \frac{p(x|y, z) \cdot p(y, z)}{q(y, z | x)} \\
            &= E_{q(y,z | x)}\log p(x | y,z) - D_{KL}[q(y,z | x) || p(y,z)]
\end{aligned}
\end{equation}</script><p>However, the joint distribution $p(y,z)$ cannot disentangle $y$ and $z$ <sup><a href="#fn_1" id="reffn_1">1</a></sup>, since every dimension shows the joint probability (y, z) and we cannot know the separate contribution.  </p>
<h2 id="Conditional-VAE-on-Shape-with-Latent-Appearance"><a href="#Conditional-VAE-on-Shape-with-Latent-Appearance" class="headerlink" title="Conditional VAE on Shape with Latent Appearance"></a>Conditional VAE on Shape with Latent Appearance</h2><p>In the previous section we have shown that a standard VAE with two latent variables is not suitable for learning disentangled representations of $y$ and $z$. Therefore, we assume that we have an estimator function for the variable $y$, i.e., $\hat{y} = e(x)$. For example, e could provide information by extracting edges or estimating human joint positions. Now, our prior becomes $p(z | \hat{y})$ and we wish to maximize $p(x | \hat{y})$. We have </p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\log p(x | \hat{y})    &= \log \int p(x, z| \hat{y}) dz \\
                    &= \log \int \frac{p(x, z| \hat{y})}{q(z | x, \hat{y})} q(z | x, \hat{y}) dz \\ 
                    &= \log E_{q(z | x, \hat{y})} \frac{p(x, z| \hat{y})}{q(z | x, \hat{y})} \\ 
                    &= \log E_{q(z | x, \hat{y})} \frac{p(x|z, \hat{y}) \cdot p(z| \hat{y})}{q(z | x, \hat{y})} \\ 
                    &\geq E_{q(z | x, \hat{y})} \log \frac{p(x|z, \hat{y}) \cdot p(z| \hat{y})}{q(z | x, \hat{y})} \\ 
                    &= E_{q(z | x, \hat{y})} \log p(x |\hat{y}, z) - D_{KL}[q(z | x, \hat{y}) || p(z| \hat{y})]

\end{aligned}
\end{equation}</script><p>This can also capture some interrelations between shape and appearance. For instance, a person wearing a dress is less likely to be running. We have our loss as </p>
<script type="math/tex; mode=display">
L(x, \theta, \phi) = E_{q(z | x, \hat{y})} \log p(x |\hat{y}, z) - D_{KL}[q(z | x, \hat{y}) || p(z| \hat{y})]</script><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>We assume further that the distribution $p(x | \hat{y}, z)$ has standard deviation and the function $G_{\theta}(\hat{y}, z)$ is a deterministic function in $\hat{y}$. We define our reconstruction loss as</p>
<script type="math/tex; mode=display">
L_{rec}(x, \theta, \phi) = ||x - G_{\theta}(\hat{y}, z)||_{1}.</script><p>However, it is well known that pixelwise statistics of images do not model perceptual quality of images well. Instead, we adopt the perceptual loss and formuulate the final loss as </p>
<script type="math/tex; mode=display">
L(x, \theta, \phi) = \sum_{k} {\lambda_{k}}|| \phi_{k}(x) - \phi_{k}(G_{\theta}(\hat{y}, z)) ||_{1} - D_{KL}[q(z | x, \hat{y}) || p(z| \hat{y})]</script><p>where $\phi$ is a network for measuring perceptual similarity. </p>
<h2 id="Disentanglement"><a href="#Disentanglement" class="headerlink" title="Disentanglement"></a>Disentanglement</h2><p>The appearance $z$ is sampled from a Gaussian distribution whose parameters are estimated by an encoding network $F_{\phi}$. The architecture of the entire network is illustrated as follows. </p>
<center><img src="/2019/02/08/paper-notes-variational-u-net/cond_u_net_architecture.jpg" width="500"></center>

<p>The optimization requires balancing two terms, namely the likelihood and the KL divergence. If z has obtained shape information from $x$ in addition to those from $\hat{y}$, then the devition of $q(z | \hat{y}, z)$ from the prior $p(\hat{y}, z)$ will exacerbate. However, little information coming from $x$ can lead to a undesirable likelihood. Therefore, any additional information about the shape encode in $z$, which is not already contained in the prior, incurs a cost without providing new information on the likelihood. </p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><h2 id="Person-Re-Identification-Metric"><a href="#Person-Re-Identification-Metric" class="headerlink" title="Person Re-Identification Metric"></a>Person Re-Identification Metric</h2><p>This paper develops a metric for evaluating the disentanglement extent of shape and appearance. They trained a person re-identification algorithm and use that algorithm to extract features of images generated with the same shape yet different appearances. We then evaluate the distances among these extracted features. </p>
<h2 id="Deepfashion"><a href="#Deepfashion" class="headerlink" title="Deepfashion"></a>Deepfashion</h2><center>
<img src="/2019/02/08/paper-notes-variational-u-net/deepfashion_testing_cond_0100000.png" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/deepfashion_testing_test_sample_0100000.png" alt="Alpha GAN Architecture" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/deepfashion_transfer_0100000.png" alt="Alpha GAN Architecture" width="200" hspace="10">
</center>

<h2 id="Marketing"><a href="#Marketing" class="headerlink" title="Marketing"></a>Marketing</h2><center>
<img src="/2019/02/08/paper-notes-variational-u-net/market_testing_cond_0100000.png" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/market_testing_test_sample_0100000.png" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/market_transfer_0100000.png" width="200" hspace="10">
</center>

<h2 id="COCO"><a href="#COCO" class="headerlink" title="COCO"></a>COCO</h2><center>
<img src="/2019/02/08/paper-notes-variational-u-net/coco_testing_cond_0100000.png" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/coco_testing_test_sample_0100000.png" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/coco_transfer_0100000.png" width="200" hspace="10">
</center>

<h1 id="Highlights"><a href="#Highlights" class="headerlink" title="Highlights"></a>Highlights</h1><ol>
<li>The proposed model does not require data of the same object with varying pose or appearance. </li>
<li>Appearance can be sampled from its distribution. </li>
</ol>
<h1 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h1><p>The quality of generated images depends highly on the dataset used for training. Although the method in this paper is not limited by the availability of labeled images showing the same appearance in different poses, it requires the dataset to provide sufficient appearance details, one way of which can be to have different appearances of the same shape. </p>
<p>The COCO dataset shows large variance in both visual qualities (e.g., light conditioning, resolutions, and so on) as well as in appearance. This leads to little overlap of appearance details in poses. As a consequence, the model will fail to learn fine details of appearance minutiaes of image and generates low-quality images. </p>
<p>The following figure exhibits some failure cases: </p>
<ol>
<li>Part of the baby is missing due to the rare data of babies. </li>
<li>Objects in different scales lead to a poor appearance transformation. </li>
<li>Different pose distribution over different characteristics of images. For example, males and females have conspicuous differences. </li>
<li>Under heavly viewpoint changes, appearance can be entirely unrelated. The algorithm, however, will assume both views are related. </li>
<li>The paper says <code>our model is confused if occluded body parts are annotated since this is not the case for most training samples</code>. I don’t understand. </li>
</ol>
<center> <img src="/2019/02/08/paper-notes-variational-u-net/failure_cases.png" width="700" hspace="10"> </center>

<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><p><sup><a href="#fn_1" id="reffn_1">1</a></sup> What is the condition for disentangling shape and appearance. The paper writes as <em>A standard normal prior can model $z$ but it is not suited to describe the spatial information contained in $y$, which is localized and easily gets lost in the bottleneck</em>. What does that <em>localized and easily gets lost in the bottleneck</em> mean? </p>
</div></header><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a class="post__tag__link" href="/tags/Paper-Notes/">Paper Notes</a></li></ul></footer></article><section class="reward"><a class="btn-reward" href="#">打赏</a><div class="reward-wrapper clearfix"><img src="/img/wechat.png" title="支付宝"></div></section><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="//cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></main><footer class="foot"><div class="foot-copy">&copy; 2016 - 2019 Zhenyue Qin</div></footer><script src="/js/scroller.js"></script><script src="/js/main.js"></script></body></html>