<!DOCTYPE html>


  <html class="light page-post">


<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Paper Notes on A Variational U-Net for Conditional Appearance and Shape Generation | Deep Mental Artificial Intelligence</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="Paper Notes,">
  

  <meta name="description" content="OverviewThis paper develops a conditional U-Net for shape-guided image generation, employing variational inference for appearance. That is, it disentangles an image into shape and appearance.  Methods">
<meta name="keywords" content="Paper Notes">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Notes on A Variational U-Net for Conditional Appearance and Shape Generation">
<meta property="og:url" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/index.html">
<meta property="og:site_name" content="Deep Mental Artificial Intelligence">
<meta property="og:description" content="OverviewThis paper develops a conditional U-Net for shape-guided image generation, employing variational inference for appearance. That is, it disentangles an image into shape and appearance.  Methods">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/cond_u_net_architecture.jpg">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/deepfashion_testing_cond_0100000.png">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/deepfashion_testing_test_sample_0100000.png">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/deepfashion_transfer_0100000.png">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/market_testing_cond_0100000.png">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/market_testing_test_sample_0100000.png">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/market_transfer_0100000.png">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/coco_testing_cond_0100000.png">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/coco_testing_test_sample_0100000.png">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/coco_transfer_0100000.png">
<meta property="og:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/failure_cases.png">
<meta property="og:updated_time" content="2019-04-01T07:00:31.713Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Paper Notes on A Variational U-Net for Conditional Appearance and Shape Generation">
<meta name="twitter:description" content="OverviewThis paper develops a conditional U-Net for shape-guided image generation, employing variational inference for appearance. That is, it disentangles an image into shape and appearance.  Methods">
<meta name="twitter:image" content="https://zhenyueqin.github.io/2019/02/08/paper-notes-variational-u-net/cond_u_net_architecture.jpg">

  

  
    <link rel="icon" href="/images/general/Hua-Round.png">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  

  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  

</head>
</html>
<body>


  
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/archives/" rel="noopener noreferrer" target="_self">
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/category/" rel="noopener noreferrer" target="_self">
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/tag/" rel="noopener noreferrer" target="_self">
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/link/" rel="noopener noreferrer" target="_self">
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/about/" rel="noopener noreferrer" target="_self">
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a class="CIRCLE" href="/search/" rel="noopener noreferrer" target="_self">
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>




<div class="content content-post CENTER">
   <article id="post-paper-notes-variational-u-net" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">Paper Notes on A Variational U-Net for Conditional Appearance and Shape Generation</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2019.02.08</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Zhenyue Qin</span>
        </span>
      

      
  <span class="article-category">
    <i class="icon-list"></i>
    <a class="article-category-link" href="/categories/Computer-Vision/">Computer Vision</a>
  </span>



      

      
      
    </div>
  </header>

  <div class="article-content">
    
      <h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>This paper develops a conditional U-Net for shape-guided image generation, employing variational inference for appearance. That is, it disentangles an image into shape and appearance. </p>
<h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Variational-Autoencoder-with-Latent-Shape-and-Appearance"><a href="#Variational-Autoencoder-with-Latent-Shape-and-Appearance" class="headerlink" title="Variational Autoencoder with Latent Shape and Appearance"></a>Variational Autoencoder with Latent Shape and Appearance</h2><p>We have the Evidence Lower Bound Observation (ELBO), with Jenson’s Inequality</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\log p(x)    &= \log \int p(x, y, z) dy dz \\
            &= \log \int \frac{p(x, y, z)}{q(y, z | x)} q(y, z | x) dy dz \\ 
            &= \log E_{q(y,z | x)} \frac{p(x, y, z)}{q(y, z | x)} \\ 
            &\geq E_{q(y,z | x)} \log \frac{p(x, y, z)}{q(y, z | x)} \\ 
            &= E_{q(y,z | x)} \log \frac{p(x|y, z) \cdot p(y, z)}{q(y, z | x)} \\
            &= E_{q(y,z | x)}\log p(x | y,z) - D_{KL}[q(y,z | x) || p(y,z)]
\end{aligned}
\end{equation}</script><p>However, the joint distribution $p(y,z)$ cannot disentangle $y$ and $z$ <sup><a href="#fn_1" id="reffn_1">1</a></sup>, since every dimension shows the joint probability (y, z) and we cannot know the separate contribution.  </p>
<h2 id="Conditional-VAE-on-Shape-with-Latent-Appearance"><a href="#Conditional-VAE-on-Shape-with-Latent-Appearance" class="headerlink" title="Conditional VAE on Shape with Latent Appearance"></a>Conditional VAE on Shape with Latent Appearance</h2><p>In the previous section we have shown that a standard VAE with two latent variables is not suitable for learning disentangled representations of $y$ and $z$. Therefore, we assume that we have an estimator function for the variable $y$, i.e., $\hat{y} = e(x)$. For example, e could provide information by extracting edges or estimating human joint positions. Now, our prior becomes $p(z | \hat{y})$ and we wish to maximize $p(x | \hat{y})$. We have </p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{aligned}
\log p(x | \hat{y})    &= \log \int p(x, z| \hat{y}) dz \\
                    &= \log \int \frac{p(x, z| \hat{y})}{q(z | x, \hat{y})} q(z | x, \hat{y}) dz \\ 
                    &= \log E_{q(z | x, \hat{y})} \frac{p(x, z| \hat{y})}{q(z | x, \hat{y})} \\ 
                    &= \log E_{q(z | x, \hat{y})} \frac{p(x|z, \hat{y}) \cdot p(z| \hat{y})}{q(z | x, \hat{y})} \\ 
                    &\geq E_{q(z | x, \hat{y})} \log \frac{p(x|z, \hat{y}) \cdot p(z| \hat{y})}{q(z | x, \hat{y})} \\ 
                    &= E_{q(z | x, \hat{y})} \log p(x |\hat{y}, z) - D_{KL}[q(z | x, \hat{y}) || p(z| \hat{y})]

\end{aligned}
\end{equation}</script><p>This can also capture some interrelations between shape and appearance. For instance, a person wearing a dress is less likely to be running. We have our loss as </p>
<script type="math/tex; mode=display">
L(x, \theta, \phi) = E_{q(z | x, \hat{y})} \log p(x |\hat{y}, z) - D_{KL}[q(z | x, \hat{y}) || p(z| \hat{y})]</script><h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>We assume further that the distribution $p(x | \hat{y}, z)$ has standard deviation and the function $G_{\theta}(\hat{y}, z)$ is a deterministic function in $\hat{y}$. We define our reconstruction loss as</p>
<script type="math/tex; mode=display">
L_{rec}(x, \theta, \phi) = ||x - G_{\theta}(\hat{y}, z)||_{1}.</script><p>However, it is well known that pixelwise statistics of images do not model perceptual quality of images well. Instead, we adopt the perceptual loss and formuulate the final loss as </p>
<script type="math/tex; mode=display">
L(x, \theta, \phi) = \sum_{k} {\lambda_{k}}|| \phi_{k}(x) - \phi_{k}(G_{\theta}(\hat{y}, z)) ||_{1} - D_{KL}[q(z | x, \hat{y}) || p(z| \hat{y})]</script><p>where $\phi$ is a network for measuring perceptual similarity. </p>
<h2 id="Disentanglement"><a href="#Disentanglement" class="headerlink" title="Disentanglement"></a>Disentanglement</h2><p>The appearance $z$ is sampled from a Gaussian distribution whose parameters are estimated by an encoding network $F_{\phi}$. The architecture of the entire network is illustrated as follows. </p>
<center><img src="/2019/02/08/paper-notes-variational-u-net/cond_u_net_architecture.jpg" width="500"></center>

<p>The optimization requires balancing two terms, namely the likelihood and the KL divergence. If z has obtained shape information from $x$ in addition to those from $\hat{y}$, then the devition of $q(z | \hat{y}, z)$ from the prior $p(\hat{y}, z)$ will exacerbate. However, little information coming from $x$ can lead to a undesirable likelihood. Therefore, any additional information about the shape encode in $z$, which is not already contained in the prior, incurs a cost without providing new information on the likelihood. </p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><h2 id="Person-Re-Identification-Metric"><a href="#Person-Re-Identification-Metric" class="headerlink" title="Person Re-Identification Metric"></a>Person Re-Identification Metric</h2><p>This paper develops a metric for evaluating the disentanglement extent of shape and appearance. They trained a person re-identification algorithm and use that algorithm to extract features of images generated with the same shape yet different appearances. We then evaluate the distances among these extracted features. </p>
<h2 id="Deepfashion"><a href="#Deepfashion" class="headerlink" title="Deepfashion"></a>Deepfashion</h2><center>
<img src="/2019/02/08/paper-notes-variational-u-net/deepfashion_testing_cond_0100000.png" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/deepfashion_testing_test_sample_0100000.png" alt="Alpha GAN Architecture" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/deepfashion_transfer_0100000.png" alt="Alpha GAN Architecture" width="200" hspace="10">
</center>

<h2 id="Marketing"><a href="#Marketing" class="headerlink" title="Marketing"></a>Marketing</h2><center>
<img src="/2019/02/08/paper-notes-variational-u-net/market_testing_cond_0100000.png" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/market_testing_test_sample_0100000.png" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/market_transfer_0100000.png" width="200" hspace="10">
</center>

<h2 id="COCO"><a href="#COCO" class="headerlink" title="COCO"></a>COCO</h2><center>
<img src="/2019/02/08/paper-notes-variational-u-net/coco_testing_cond_0100000.png" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/coco_testing_test_sample_0100000.png" width="200" hspace="10">

<img src="/2019/02/08/paper-notes-variational-u-net/coco_transfer_0100000.png" width="200" hspace="10">
</center>

<h1 id="Highlights"><a href="#Highlights" class="headerlink" title="Highlights"></a>Highlights</h1><ol>
<li>The proposed model does not require data of the same object with varying pose or appearance. </li>
<li>Appearance can be sampled from its distribution. </li>
</ol>
<h1 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h1><p>The quality of generated images depends highly on the dataset used for training. Although the method in this paper is not limited by the availability of labeled images showing the same appearance in different poses, it requires the dataset to provide sufficient appearance details, one way of which can be to have different appearances of the same shape. </p>
<p>The COCO dataset shows large variance in both visual qualities (e.g., light conditioning, resolutions, and so on) as well as in appearance. This leads to little overlap of appearance details in poses. As a consequence, the model will fail to learn fine details of appearance minutiaes of image and generates low-quality images. </p>
<p>The following figure exhibits some failure cases: </p>
<ol>
<li>Part of the baby is missing due to the rare data of babies. </li>
<li>Objects in different scales lead to a poor appearance transformation. </li>
<li>Different pose distribution over different characteristics of images. For example, males and females have conspicuous differences. </li>
<li>Under heavly viewpoint changes, appearance can be entirely unrelated. The algorithm, however, will assume both views are related. </li>
<li>The paper says <code>our model is confused if occluded body parts are annotated since this is not the case for most training samples</code>. I don’t understand. </li>
</ol>
<center> <img src="/2019/02/08/paper-notes-variational-u-net/failure_cases.png" width="700" hspace="10"> </center>

<h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><p><sup><a href="#fn_1" id="reffn_1">1</a></sup> What is the condition for disentangling shape and appearance. The paper writes as <em>A standard normal prior can model $z$ but it is not suited to describe the spatial information contained in $y$, which is localized and easily gets lost in the bottleneck</em>. What does that <em>localized and easily gets lost in the bottleneck</em> mean? </p>

    
  </div>

</article>


   

   



</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">Close</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/archives/" rel="noopener noreferrer" target="_self">
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/category/" rel="noopener noreferrer" target="_self">
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/tag/" rel="noopener noreferrer" target="_self">
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/link/" rel="noopener noreferrer" target="_self">
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/about/" rel="noopener noreferrer" target="_self">
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a class="CIRCLE" href="/search/" rel="noopener noreferrer" target="_self">
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <div id="comment" class="vcomment"></div>
    <script>
        var notify = 'true' == true ? true : false;
        var verify = 'true' == true ? true : false;
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
            return GUEST_INFO.indexOf(item) > -1
        });
        guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
        window.valine = new Valine({
            el: '.vcomment',
            notify: notify,
            verify: verify,
            appId: "",
            appKey: "",
            avatar:'mm',
            placeholder: "Just go go",
            guest_info:guest_info,
            pageSize:'10'
        });
    </script>
  
    

  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
